{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXglL5e53dGPT+BsvUjLCh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbloewencolon/Analyzing-The-Doctrine-of-Discovery/blob/main/Analyzing_the_DoD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to conduct a language analysis of the documents comprising the \"Doctrine of Christian Discovery.\" Despite the dataset's limited size, this analysis will delve into sentiment analysis, topic modeling, and comparative text analysis to gain insights into historical perspectives on indigenous peoples, gold, and religious matters."
      ],
      "metadata": {
        "id": "8fkq5XiQraz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setting Up the Python Environment\n",
        "First, we need to set up our Python environment with the necessary libraries."
      ],
      "metadata": {
        "id": "dS61tFaqrkI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora, models\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "GI97Yp04rco6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Collection and Preprocessing\n",
        "After sourcing the documents, we'll convert them into a text-readable format and clean the data."
      ],
      "metadata": {
        "id": "ZKH4ysvNrrGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Reading a text file\n",
        "with open('document.txt', 'r') as file:\n",
        "    document = file.read()\n",
        "\n",
        "# Basic preprocessing\n",
        "document = document.lower()\n",
        "document = nltk.word_tokenize(document)\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "document = [word for word in document if word not in stop_words]\n"
      ],
      "metadata": {
        "id": "U7bWI5gUrqjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Exploratory Data Analysis (EDA)\n",
        "We'll start with some basic EDA to understand our dataset better."
      ],
      "metadata": {
        "id": "3jvTD34orzmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Distribution\n",
        "freq_dist = nltk.FreqDist(document)\n",
        "freq_dist.plot(30, cumulative=False)\n"
      ],
      "metadata": {
        "id": "ZGwR9QTJr0SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Sentiment Analysis\n",
        "Next, we'll analyze the sentiment of the text."
      ],
      "metadata": {
        "id": "iTw5Bd9fr4G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TextBlob for sentiment analysis\n",
        "blob = TextBlob(\" \".join(document))\n",
        "print(blob.sentiment)\n",
        "\n",
        "# Using Vader Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "print(analyzer.polarity_scores(\" \".join(document)))\n"
      ],
      "metadata": {
        "id": "hvDgSCUhr4wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Topic Modeling\n",
        "We'll use LDA to identify prominent topics in the text."
      ],
      "metadata": {
        "id": "GJt_h_K4r8uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing data for LDA\n",
        "dictionary = corpora.Dictionary([document])\n",
        "corpus = [dictionary.doc2bow(text) for text in [document]]\n",
        "\n",
        "# Applying LDA\n",
        "lda_model = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "print(lda_model.print_topics())\n"
      ],
      "metadata": {
        "id": "rTp3r2j4r9ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Comparative Text Analysis\n",
        "We'll compare the frequency and context of key words."
      ],
      "metadata": {
        "id": "gGnqpUZlsB0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of comparative analysis\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform([\" \".join(document)])\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "print(word_freq)\n"
      ],
      "metadata": {
        "id": "ZMj6bKojsIDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Visualizations and Reporting\n",
        "We'll create visualizations to effectively communicate our findings."
      ],
      "metadata": {
        "id": "cjyvJ2OQsOya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Creating a Word Cloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud().generate(\" \".join(document))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "13oyBg_6sOLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Further Research and Analysis\n",
        "Suggestions for further research and analysis go here.\n",
        "\n",
        "Step 10: Ethical Considerations\n",
        "Reflect on the ethical aspects of analyzing such historical texts.\n",
        "\n",
        "Step 11: Limitations and Future Work\n",
        "Acknowledge the limitations due to the small size of the dataset and propose future research directions."
      ],
      "metadata": {
        "id": "Wz0ysQNasWRl"
      }
    }
  ]
}